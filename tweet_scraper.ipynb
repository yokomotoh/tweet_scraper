{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Sep 17 12:16:43 2019\n",
    "@author: yoko\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "# scrape the offcial Twitter accounts of Dow components\n",
    "# Output: json, csv, json line\n",
    "# Combine the total favorates and retweets per company per day\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tweepy  # https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "import json\n",
    "import sys, os\n",
    "#import argparse\n",
    "from datetime import date, timedelta, datetime\n",
    "import hashlib\n",
    "#from joblib.test.test_parallel import consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#companies = \"3M,American Express,Apple,Boeing,Caterpillar,Chevron,Cisco Systems,\" \\\n",
    "#            \"Coca-Cola,DowDuPont,ExxonMobil,General Electric,Goldman Sachs,IBM,\" \\\n",
    "#            \"Intel,Johnson & Johnson,JPMorgan Chase,McDonald's,Merck,Microsoft,Nike,\" \\\n",
    "#            \"Pfizer,Procter & Gamble,The Home Depot,Travelers,United Technologies,\" \\\n",
    "#            \"UnitedHealth Group,Verizon,Visa,Walmart,Walt Disney\".split(\",\")\n",
    "#\n",
    "#accounts = \"3M,AmericanExpress,AppleSupport,Boeing,CaterpillarInc,Chevron,Cisco,\" \\\n",
    "#           \"CocaCola,DowDuPontCo,exxonmobil,generalelectric,GoldmanSachs,IBM,intel,\" \\\n",
    "#           \"JNJNews,jpmorgan,McDonalds,Merck,Microsoft,Nike,pfizer,ProcterGamble,\" \\\n",
    "#           \"HomeDepot,Travelers,UTC,UnitedHealthGrp,verizon,Visa,Walmart,DisneyStudios\".split(\",\")\n",
    "\n",
    "companies = \"Google,Facebook,Apple,Amazon,Microsoft,Vins Fins Motohama\".split(\",\")\n",
    "accounts = \"Google,facebook,AppleSupport,amazon,microsoft,vinsfinsmotoham\".split(\",\")\n",
    "\n",
    "comDic = dict(zip(accounts, companies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name, since, until, limit, consumer_key, consumer_secret, access_key, access_secret):\n",
    "    # Twitter only allows access to a users most recent 3200 tweets with this method\n",
    "\n",
    "    print(\"collecting tweets from: \" + screen_name)\n",
    "    # oldest = \"925854293512589312\"\n",
    "    # authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "\n",
    "    # make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name=screen_name, count=limit)\n",
    "\n",
    "    # save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    # save the id of the oldest tweet less one\n",
    "#    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    if alltweets==[] :\n",
    "        oldest = 0\n",
    "        #print(\"No tweets\")\n",
    "    else:\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        #print(\"id of the oldest tweet + 1:\", alltweets[-1].id)\n",
    "\n",
    "    # keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        # all subsequent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name=screen_name, count=limit,\n",
    "                                       max_id=oldest, exclude_replies=True)\n",
    "\n",
    "        # save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        # update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print (\"...%s tweets downloaded\" % (len(alltweets)))\n",
    "\n",
    "    # transform the tweepy tweets into a 2D array that will populate the csv\n",
    "    outtweets = [[\n",
    "        tweet.created_at,\n",
    "        comDic[screen_name],\n",
    "        \"https://twitter.com/\" + screen_name + \"/status/\" + tweet.id_str,\n",
    "        tweet.text.encode(\"utf-8\"),\n",
    "        tweet.retweet_count,\n",
    "        tweet.favorite_count,\n",
    "        tweet.id_str]\n",
    "        for tweet in alltweets]\n",
    "\n",
    "    # write the csv\n",
    "    print(\"writing the csv for: %s\" % screen_name)\n",
    "    result_dir = 'csv/'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)    \n",
    "    with open('./%s%s.csv' % (result_dir, screen_name), 'w', encoding='utf8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(\n",
    "            [\"created_at\", \"company\", \"url\", \"text\", \"retweets\", \"likes\", \"id\"])\n",
    "        writer.writerows(outtweets)\n",
    "\n",
    "    # compose dictionary for outputting json\n",
    "    data = []\n",
    "    for tweet in outtweets:\n",
    "        row = {\n",
    "            \"created_at\": str(tweet[0]),\n",
    "            \"company\": str(tweet[1]),\n",
    "            \"url\": str(tweet[2]),\n",
    "            \"text\": str(tweet[3]),\n",
    "            \"re_tweets\": str(tweet[4]),\n",
    "            \"likes\": int(tweet[5]),\n",
    "            \"id\": str(tweet[6])\n",
    "        }\n",
    "        data.append(row)\n",
    "\n",
    "    # write the json\n",
    "    result_dir = 'json/'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    with open('./%s%s.json' % (result_dir, screen_name), 'w', encoding='utf8') as outfile:\n",
    "        json.dump(list(data), outfile)\n",
    "\n",
    "    return outtweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jlFile = open(\"twitter.jl\", \"w\")\n",
    "\n",
    "docID = 0  # auto increasing doc ID\n",
    "rawContent = \"<!DOCTYPEhtml><html><head><meta charset='UTF-8'>\" \\\n",
    "             \"<title>%s</title></head><body>\" \\\n",
    "             \"<ul><li class='company'>%s</li>\" \\\n",
    "             \"<li class='url'>%s</li>\" \\\n",
    "             \"<li class='text'>%s</li>\" \\\n",
    "             \"<li class='creat_time'>%s</li>\" \\\n",
    "             \"<li class='likes'>%s</li>\" \\\n",
    "             \"<li class='retweets'>%s</li>\" \\\n",
    "             \"</ul></body></html>\"\n",
    "\n",
    "\n",
    "def writeJL(line):\n",
    "    global docID\n",
    "    docID += 1\n",
    "    row = {\n",
    "        \"doc_id\": hashlib.sha256(str(line[2]).encode('utf-8')).hexdigest(),\n",
    "        \"timestamp_crawl\": str(line[0]),\n",
    "        \"url\": str(line[2]),\n",
    "        \"raw_content\": rawContent % (\"tweets of \" + str(line[1]) + \" on \" + str(line[0]),\n",
    "                                     str(line[1]), str(line[2]), str(line[3]), str(line[0]),\n",
    "                                      str(line[5]), str(line[4]))\n",
    "    }\n",
    "    jlFile.write(json.dumps(row, separators=(',', ': ')) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set crawler target and parameters.\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#\n",
    "#    parser.add_argument('-s', '--since', help='Set the start date you want to crawling. Format: \\'yyyymmdd\\'')\n",
    "#    parser.add_argument('-u', '--until', help='Set the end date you want to crawling. Format: \\'yyyymmdd\\'')\n",
    "#    parser.add_argument('-l', '--limit', help='This is the maximum number of messages, limitation is 200')\n",
    "#\n",
    "#    parser.add_argument('-ck', '--c_key', help='consumer key')    \n",
    "#    parser.add_argument('-cs', '--c_secret', help='consumer secret')\n",
    "#    parser.add_argument('-ak', '--a_key', help='access key')    \n",
    "#    parser.add_argument('-as', '--a_secret', help='access secret')        \n",
    "#\n",
    "#         \n",
    "#    parser.print_help()\n",
    "#    args = parser.parse_args()\n",
    "\n",
    "#    since = str(args.since)\n",
    "#    until = str(args.until)\n",
    "    since = str(\"20190912\") #Set the start date you want to crawling. Format: \\'yyyymmdd\\'\n",
    "    until = str(\"20190919\") #Set the end date you want to crawling. Format: \\'yyyymmdd\\'\n",
    "    limit = int(str(\"10\")) #This is the maximum number of messages, limitation is 200\n",
    "\n",
    "#    if args.limit is not None:\n",
    "#        limit = int(args.limit)\n",
    "#        if limit > 200: limit = 200\n",
    "#    else:\n",
    "#        limit = 200\n",
    "    if limit > 200:\n",
    "        limit = 200\n",
    "\n",
    "#    consumer_key = str(args.c_key)\n",
    "#    consumer_secret = str(args.c_secret)\n",
    "#    access_key = str(args.a_key)\n",
    "#    access_secret = str(args.a_secret)    \n",
    "    consumer_key = '0pW44wwmZ3ft7FXAf5JZxPW8y'\n",
    "    consumer_secret = 'YJxLt7QER3O3Gvd0JerTgzZaKHssQxL51eBbnQUK3r0SYWm6vL'\n",
    "    access_key = '212782440-AodAkix83Py5tjj9uyPSPfigBIOZLT2fRks2NEuA'\n",
    "    access_secret = 'qyieBwlIONEKXw78wvqAhELQDKleUHs9KHHyGeYgCOFIn'\n",
    "\n",
    "    \n",
    "    print ('yyyy=%d, mm=%d, dd=%d'%(int(since[:4]), int(since[4:6]), int(since[6:8])))\n",
    "    d1 = date(int(since[:4]), int(since[4:6]), int(since[6:8]))\n",
    "    d2 = date(int(until[:4]), int(until[4:6]), int(until[6:8]))\n",
    "    # generate a list containing all the dates in between d1 and d2\n",
    "    dateRange = [d1 + timedelta(days=x) for x in range((d2 - d1).days + 1)]\n",
    "    \n",
    "    allData = {}\n",
    "    for acc in accounts:\n",
    "        allData[comDic[acc]] = get_all_tweets(acc, since, until, limit, consumer_key, consumer_secret, access_key, access_secret)\n",
    "    \n",
    "    csvLst = []\n",
    "    for date in dateRange:\n",
    "        for com in companies:\n",
    "            retweets = 0\n",
    "            likes = 0\n",
    "            for tweet in allData[com]:\n",
    "                if tweet[0].date() == date:\n",
    "                    writeJL(tweet)  # write one json line\n",
    "                    retweets += int(tweet[4])\n",
    "                    likes += int(tweet[5])\n",
    "            csvLst.append([date, com, retweets, likes])\n",
    "    \n",
    "    print(\"##### writing the .csv file in a time sequence #####\")\n",
    "    with open(\"twitter.csv\", \"w\", encoding='utf8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"date\", \"company\", \"re_tweets\", \"likes\"])\n",
    "        writer.writerows(csvLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tweet_scraper.ipynb to html\n",
      "[NbConvertApp] Writing 307169 bytes to tweet_scraper.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html tweet_scraper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
